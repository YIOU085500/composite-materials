import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from xgboost import XGBRegressor
from sklearn.model_selection import RepeatedKFold
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import MinMaxScaler
import os
import joblib
from io import BytesIO
import time
from datetime import timedelta

# ==============================================================================
# 1. ä»æ‚¨æä¾›çš„è„šæœ¬ä¸­æ•´åˆçš„æ ¸å¿ƒå‡½æ•° (ç¨ä½œä¿®æ”¹ä»¥é€‚åº”Streamlit)
# ==============================================================================

# --- æ•°æ®é¢„å¤„ç†å‡½æ•° ---
def clean_dataframe(df):
    """æ¸…ç†DataFrameä¸­å¯èƒ½å­˜åœ¨çš„ç©ºåˆ—æˆ–æ ¼å¼é—®é¢˜ã€‚"""
    df.dropna(axis=1, how='all', inplace=True)
    unnamed_cols = [col for col in df.columns if 'Unnamed' in col]
    if unnamed_cols:
        df.drop(columns=unnamed_cols, inplace=True)
    return df

def calculate_weights_3D(df, norm_cols, bins_per_dim=4):
    """æ ¹æ®ä¸‰ä¸ªå½’ä¸€åŒ–åçš„ç»´åº¦ï¼Œé€šè¿‡3Dåˆ†ç®±æ¥è®¡ç®—æ ·æœ¬æƒé‡ã€‚"""
    temp_df = df.copy()
    binned_cols = []
    for i, col in enumerate(norm_cols):
        binned_col_name = f'bin_{i}'
        temp_df[binned_col_name] = pd.cut(temp_df[col], bins=bins_per_dim, labels=False, include_lowest=True)
        binned_cols.append(binned_col_name)
    
    group_sizes = temp_df.groupby(binned_cols)[binned_cols[0]].transform('size')
    weights = 1.0 / group_sizes
    return weights

def process_data_for_training(df):
    """
    åŠ è½½DataFrameï¼Œè¿›è¡Œå½’ä¸€åŒ–å’Œæƒé‡åˆ†é…ï¼Œè¿”å›å¤„ç†åçš„DataFrameå’Œscalerã€‚
    """
    df = clean_dataframe(df.copy())
    
    feature_cols = ['è€åŒ–æ¸©åº¦', 'è€åŒ–æ¹¿åº¦', 'è€åŒ–æ—¶é—´']
    if not all(col in df.columns for col in feature_cols):
        st.error(f"é”™è¯¯ï¼šä¸Šä¼ çš„æ–‡ä»¶ä¸­ç¼ºå°‘å¿…è¦çš„åˆ—ã€‚éœ€è¦ {feature_cols}ï¼Œä½†åªæ‰¾åˆ°äº† {list(df.columns)}ã€‚")
        return None, None
        
    scaler = MinMaxScaler()
    normalized_features = scaler.fit_transform(df[feature_cols])
    normalized_cols = [f'{col}_normalized' for col in feature_cols]
    df[normalized_cols] = normalized_features

    sample_weights = calculate_weights_3D(df, norm_cols=normalized_cols, bins_per_dim=4)
    df['sample_weight'] = sample_weights
    
    return df, scaler

# --- æœºå™¨å­¦ä¹ ä¸å¯è§†åŒ–å‡½æ•° ---
def train_and_visualize_model(df, features, target, title, mode='é«˜ç²¾åº¦'):
    """
    æ‰§è¡Œäº¤å‰éªŒè¯ï¼Œè®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼Œå¹¶ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ã€‚
    mode: 'å¿«é€Ÿ' æˆ– 'é«˜ç²¾åº¦'
    è¿”å›: model, scaler, figure, metrics
    """
    # æ•°æ®é‡æ£€æŸ¥
    if df is None or df.empty:
        st.error("æ²¡æœ‰æ•°æ®å¯ä¾›è®­ç»ƒï¼Œè¯·ä¸Šä¼ æœ‰æ•ˆçš„æ•°æ®æ–‡ä»¶ã€‚")
        return None, None, None, None
    
    # æ ¹æ®æ•°æ®é‡ç»™å‡ºå»ºè®®
    sample_count = len(df)
    if sample_count < 10:
        st.error(f"æ•°æ®é‡ä¸¥é‡ä¸è¶³ ({sample_count}ä¸ªæ ·æœ¬)ï¼Œæ— æ³•è¿›è¡Œå¯é çš„è®­ç»ƒï¼Œè¯·ä¸Šä¼ è‡³å°‘10ä¸ªæ ·æœ¬ã€‚")
        return None, None, None, None
    elif sample_count < 20:
        st.warning(f"æ•°æ®é‡è¾ƒå°‘ ({sample_count}ä¸ªæ ·æœ¬)ï¼Œæ¨¡å‹å¯èƒ½ä¸å¤Ÿç¨³å®šï¼Œå»ºè®®ä¸Šä¼ è‡³å°‘20ä¸ªæ ·æœ¬ä»¥è·å¾—æ›´å¯é çš„ç»“æœã€‚")
    elif sample_count < 50:
        st.info(f"å½“å‰æ•°æ®é‡ ({sample_count}ä¸ªæ ·æœ¬) åŸºæœ¬æ»¡è¶³è®­ç»ƒéœ€æ±‚ï¼Œä½†æ›´å¤šçš„æ•°æ®ä¼šå¸¦æ¥æ›´å¥½çš„æ¨¡å‹æ€§èƒ½ã€‚")
    else:
        st.success(f"æ•°æ®é‡å……è¶³ ({sample_count}ä¸ªæ ·æœ¬)ï¼Œéå¸¸é€‚åˆè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼")

    X = df[features]
    y = df[target]
    
    # æ ¹æ®æ¨¡å¼è®¾ç½®å‚æ•°
    if mode == 'å¿«é€Ÿ':
        n_splits = 3
        n_repeats = 2
        n_estimators = 100
        grid_resolution = 20
        st.info("âš¡ å¿«é€Ÿæ¨¡å¼ï¼šä½¿ç”¨ç®€åŒ–å‚æ•°è¿›è¡Œè®­ç»ƒï¼Œé€Ÿåº¦æ›´å¿«ä½†ç²¾åº¦å¯èƒ½ç•¥ä½ã€‚")
    else:  # é«˜ç²¾åº¦æ¨¡å¼
        n_splits = 5
        n_repeats = 5
        n_estimators = 200
        grid_resolution = 30
        st.info("ğŸ” é«˜ç²¾åº¦æ¨¡å¼ï¼šä½¿ç”¨å®Œæ•´å‚æ•°è¿›è¡Œè®­ç»ƒï¼Œç²¾åº¦æ›´é«˜ä½†éœ€è¦æ›´é•¿æ—¶é—´ã€‚")
    
    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # è®¡ç®—æ€»æ­¥éª¤æ•°
    total_cv_steps = n_splits * n_repeats
    total_steps = total_cv_steps + 2  # +1ä¸ºæœ€ç»ˆæ¨¡å‹è®­ç»ƒ, +1ä¸ºå¯è§†åŒ–
    current_step = 0
    start_time = time.time()
    
    # æ‰§è¡Œé‡å¤KæŠ˜äº¤å‰éªŒè¯
    rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)
    r2_scores, mae_scores = [], []
    model_params = {
        'objective': 'reg:squarederror', 'n_estimators': n_estimators, 'max_depth': 5,
        'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8,
        'random_state': 42, 'n_jobs': -1
    }

    for i, (train_index, val_index) in enumerate(rkf.split(X, y)):
        # æ›´æ–°è¿›åº¦
        current_step += 1
        progress = current_step / total_steps
        progress_bar.progress(progress)
        
        # è®¡ç®—é¢„è®¡å‰©ä½™æ—¶é—´
        elapsed_time = time.time() - start_time
        estimated_total_time = elapsed_time / progress if progress > 0 else 0
        remaining_time = estimated_total_time - elapsed_time if estimated_total_time > 0 else 0
        remaining_time_str = str(timedelta(seconds=int(remaining_time)))
        
        status_text.text(f"æ­£åœ¨è¿›è¡Œäº¤å‰éªŒè¯ ({i+1}/{total_cv_steps})... é¢„è®¡å‰©ä½™æ—¶é—´: {remaining_time_str}")
        
        X_train, X_val = X.iloc[train_index], X.iloc[val_index]
        y_train, y_val = y.iloc[train_index], y.iloc[val_index]
        weights_train = df['sample_weight'].iloc[train_index]

        model_cv = XGBRegressor(**model_params)
        model_cv.fit(X_train, y_train, sample_weight=weights_train)
        y_pred = model_cv.predict(X_val)
        
        r2_scores.append(r2_score(y_val, y_pred))
        mae_scores.append(mean_absolute_error(y_val, y_pred))
    
    avg_r2, std_r2 = np.mean(r2_scores), np.std(r2_scores)
    avg_mae, std_mae = np.mean(mae_scores), np.std(mae_scores)
    metrics = {'r2': avg_r2, 'r2_std': std_r2, 'mae': avg_mae, 'mae_std': std_mae}

    # æ›´æ–°è¿›åº¦ - è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    current_step += 1
    progress = current_step / total_steps
    progress_bar.progress(progress)
    status_text.text("æ­£åœ¨è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    
    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    final_model = XGBRegressor(**model_params)
    final_model.fit(X, y, sample_weight=df['sample_weight'])

    # æ›´æ–°è¿›åº¦ - ç”Ÿæˆå¯è§†åŒ–
    current_step += 1
    progress = current_step / total_steps
    progress_bar.progress(progress)
    status_text.text("æ­£åœ¨ç”Ÿæˆ3Då“åº”é¢å¯è§†åŒ–...")
    
    # ç”Ÿæˆ3Då“åº”é¢å¯è§†åŒ–
    fig = go.Figure()
    weld_statuses = {0: {'name': 'æ— ç†”æ¥ç—•', 'color': 'blue'}, 1: {'name': 'æœ‰ç†”æ¥ç—•', 'color': 'red'}}
    
    for status_code, props in weld_statuses.items():
        subset_df = df[df['æœ‰æ— ç†”æ¥ç—•(0/1)'] == status_code]
        if not subset_df.empty:
            fig.add_trace(go.Scatter3d(
                x=subset_df[features[0]], y=subset_df[features[1]], z=subset_df[target],
                mode='markers',
                marker=dict(size=5, color=props['color'], symbol='circle' if status_code == 0 else 'diamond', opacity=0.7),
                name=f'åŸå§‹æ•°æ® ({props["name"]})'
            ))

        unique_humidity_levels = np.linspace(X[features[2]].min(), X[features[2]].max(), 3) # å¯è§†åŒ–3ä¸ªæ¹¿åº¦æ°´å¹³
        for humidity_level in unique_humidity_levels:
            temp_range = np.linspace(X[features[0]].min(), X[features[0]].max(), grid_resolution)
            time_range = np.linspace(X[features[1]].min(), X[features[1]].max(), grid_resolution)
            grid_temp, grid_time = np.meshgrid(temp_range, time_range)
            grid_humidity = np.full(grid_temp.shape, humidity_level)
            
            predict_df = pd.DataFrame({
                features[0]: grid_temp.flatten(),
                features[1]: grid_time.flatten(),
                features[2]: grid_humidity.flatten(),
                features[3]: status_code
            })
            
            predicted_strength = final_model.predict(predict_df)
            grid_strength = predicted_strength.reshape(grid_temp.shape)
            
            colorscale = 'Blues' if status_code == 0 else 'Reds'
            fig.add_trace(go.Surface(
                x=temp_range, y=time_range, z=grid_strength,
                opacity=0.7, colorscale=colorscale, showscale=False,
                name=f'æ¨¡å‹å“åº”é¢ ({props["name"]}, æ¹¿åº¦={humidity_level:.2f})'
            ))

    fig.update_layout(
        title=f'<b>{title} å¼ºåº¦é¢„æµ‹æ¨¡å‹</b><br>äº¤å‰éªŒè¯ RÂ²: {avg_r2:.3f} Â± {std_r2:.3f}',
        scene=dict(
            xaxis_title=f"{features[0].replace('_normalized', '')}",
            yaxis_title=f"{features[1].replace('_normalized', '')}",
            zaxis_title=target
        ),
        legend=dict(x=0.01, y=0.99),
        margin=dict(l=0, r=0, b=0, t=60)
    )
    
    # å®Œæˆæ‰€æœ‰æ­¥éª¤
    progress_bar.progress(1.0)
    total_time = time.time() - start_time
    status_text.text(f"âœ… æ‰€æœ‰æ­¥éª¤å·²å®Œæˆï¼æ€»è€—æ—¶: {str(timedelta(seconds=int(total_time)))}")
    
    return final_model, fig, metrics, grid_resolution

# ==============================================================================
# 2. Streamlit ç½‘é¡µåº”ç”¨ç•Œé¢ä¸é€»è¾‘
# ==============================================================================

st.set_page_config(page_title="ææ–™å¼ºåº¦é¢„æµ‹ä¸åˆ†æå¹³å°", layout="wide")

st.title("ææ–™å¼ºåº¦æ™ºèƒ½é¢„æµ‹ä¸åˆ†æå¹³å° ğŸ“ˆ")

# --- ä¾§è¾¹æ  ---
st.sidebar.header("âš™ï¸ æ“ä½œé¢æ¿")
app_mode = st.sidebar.selectbox("é€‰æ‹©æ“ä½œæ¨¡å¼", ["è®­ç»ƒæ–°æ¨¡å‹", "åŠ è½½æ¨¡å‹å¹¶é¢„æµ‹"])
model_type = st.sidebar.radio("é€‰æ‹©æ¨¡å‹ç±»å‹", ('æ‹‰ä¼¸å¼ºåº¦', 'å¼¯æ›²å¼ºåº¦'), key="model_type_selection")

# --- æ ¹æ®æ¨¡å¼é€‰æ‹©ï¼Œæ¸²æŸ“ä¸åŒé¡µé¢ ---

if app_mode == "è®­ç»ƒæ–°æ¨¡å‹":
    st.header("æ¨¡å¼ä¸€: è®­ç»ƒæ–°çš„é¢„æµ‹æ¨¡å‹")

    # 1. ä¸Šä¼ æ–‡ä»¶
    st.subheader("æ­¥éª¤ 1: ä¸Šä¼ æ‚¨çš„æ•°æ®æ–‡ä»¶")
    
    with st.expander("ç‚¹æ­¤æŸ¥çœ‹æ•°æ®æ ¼å¼è¦æ±‚", expanded=False):
        st.info(
            """
            è¯·ä¸Šä¼ ä¸€ä¸ªCSVæ–‡ä»¶ï¼Œç¡®ä¿åŒ…å«ä»¥ä¸‹**å¿…éœ€**çš„åˆ—ï¼š
            """
        )
        
        # ä½¿ç”¨è¡¨æ ¼å±•ç¤ºæ•°æ®æ ¼å¼è¦æ±‚
        data_format_df = pd.DataFrame({
            'åˆ—å': ['è€åŒ–æ¸©åº¦', 'è€åŒ–æ¹¿åº¦', 'è€åŒ–æ—¶é—´', 'æœ‰æ— ç†”æ¥ç—•(0/1)', f'{model_type}'],
            'æ•°æ®ç±»å‹': ['æ•°å€¼å‹', 'æ•°å€¼å‹', 'æ•°å€¼å‹', 'æ•´æ•°å‹', 'æ•°å€¼å‹'],
            'ç¤ºä¾‹å€¼': ['85, 90.5, 95', '75, 80, 85', '24, 48, 72', '0, 1', '120.5, 98.2'],
            'è¯´æ˜': ['æ¸©åº¦å€¼ (Â°C)', 'æ¹¿åº¦ç™¾åˆ†æ¯” (%)', 'æ—¶é—´ (å°æ—¶)', '0=æ— ç†”æ¥ç—•, 1=æœ‰ç†”æ¥ç—•', 'å¼ºåº¦å€¼ (æ ¹æ®æ‚¨é€‰æ‹©çš„æ¨¡å‹ç±»å‹)']  
        })
        
        st.table(data_format_df)
        
        st.info(
            """
            **æ³¨æ„**: 
            - æ–‡ä»¶ä¸­åªéœ€åŒ…å«å¯¹åº”æ¨¡å‹ç±»å‹çš„å¼ºåº¦åˆ—å³å¯ï¼ˆä¾‹å¦‚ï¼Œè®­ç»ƒæ‹‰ä¼¸å¼ºåº¦æ¨¡å‹æ—¶ï¼Œæ–‡ä»¶ä¸­å¿…é¡»æœ‰`æ‹‰ä¼¸å¼ºåº¦`åˆ—ï¼‰ã€‚
            - è¯·ç¡®ä¿æ•°æ®ä¸­æ²¡æœ‰ç¼ºå¤±å€¼ï¼Œæ‰€æœ‰åˆ—åå¿…é¡»ä¸ä¸Šè¡¨å®Œå…¨ä¸€è‡´ã€‚
            - å»ºè®®ä½¿ç”¨UTF-8ç¼–ç ä¿å­˜CSVæ–‡ä»¶ï¼Œä»¥é¿å…ä¸­æ–‡ä¹±ç é—®é¢˜ã€‚
            """
        )
        
        # ç¤ºä¾‹æ•°æ®
        st.subheader("ç¤ºä¾‹æ•°æ®é¢„è§ˆï¼š")
        st.dataframe(pd.DataFrame({
            'è€åŒ–æ¸©åº¦': [85, 90, 95], 'è€åŒ–æ¹¿åº¦': [75, 80, 85], 'è€åŒ–æ—¶é—´': [24, 48, 72],
            'æœ‰æ— ç†”æ¥ç—•(0/1)': [0, 1, 0], model_type: [120.5, 98.2, 115.7]
        }))

    uploaded_file = st.file_uploader(f"è¯·ä¸Šä¼ ç”¨äºè®­ç»ƒ **{model_type}** æ¨¡å‹çš„æ•°æ®", type="csv")

    # 2. è®­ç»ƒæ¨¡å¼é€‰æ‹©å’Œæ‰§è¡Œ
    if uploaded_file is not None:
        st.subheader("æ­¥éª¤ 2: é€‰æ‹©è®­ç»ƒæ¨¡å¼")
        
        # æ·»åŠ è®­ç»ƒæ¨¡å¼é€‰æ‹©
        training_mode = st.radio(
            "è¯·é€‰æ‹©è®­ç»ƒæ¨¡å¼ï¼š",
            ["å¿«é€Ÿ", "é«˜ç²¾åº¦"],
            index=1,
            help="å¿«é€Ÿæ¨¡å¼ï¼šè®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œä½†ç²¾åº¦å¯èƒ½ç•¥ä½ã€‚é«˜ç²¾åº¦æ¨¡å¼ï¼šè®­ç»ƒæ—¶é—´æ›´é•¿ï¼Œä½†ç²¾åº¦æ›´é«˜ã€‚"
        )
        
        # æ˜¾ç¤ºæ¨¡å¼è¯´æ˜
        if training_mode == "å¿«é€Ÿ":
            st.info("âš¡ **å¿«é€Ÿæ¨¡å¼**ï¼šä½¿ç”¨ç®€åŒ–çš„äº¤å‰éªŒè¯å’Œè¾ƒå°‘çš„æ¨¡å‹å‚æ•°ï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œé€‚åˆåˆæ­¥æ¢ç´¢æˆ–æ•°æ®é‡è¾ƒå°çš„æƒ…å†µã€‚")
            estimated_time = "çº¦1-3åˆ†é’Ÿ"
        else:
            st.info("ğŸ” **é«˜ç²¾åº¦æ¨¡å¼**ï¼šä½¿ç”¨å®Œæ•´çš„äº¤å‰éªŒè¯å’Œæ›´å¤šçš„æ¨¡å‹å‚æ•°ï¼Œè®­ç»ƒæ—¶é—´æ›´é•¿ï¼Œä½†æ¨¡å‹ç²¾åº¦å’Œç¨³å®šæ€§æ›´é«˜ã€‚")
            estimated_time = "çº¦3-10åˆ†é’Ÿ"
        
        # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
        if st.checkbox("é¢„è§ˆä¸Šä¼ çš„æ•°æ®"):
            try:
                uploaded_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°æ–‡ä»¶å¼€å¤´
                preview_df = pd.read_csv(uploaded_file)
                st.write(f"æ•°æ®é¢„è§ˆ (å…±{len(preview_df)}è¡Œ)ï¼š")
                st.dataframe(preview_df.head(10))
                
                # æ£€æŸ¥æ•°æ®åˆ—
                required_cols = ['è€åŒ–æ¸©åº¦', 'è€åŒ–æ¹¿åº¦', 'è€åŒ–æ—¶é—´', 'æœ‰æ— ç†”æ¥ç—•(0/1)', model_type]
                missing_cols = [col for col in required_cols if col not in preview_df.columns]
                
                if missing_cols:
                    st.warning(f"âš ï¸ è­¦å‘Šï¼šæ•°æ®ä¸­ç¼ºå°‘ä»¥ä¸‹å¿…è¦åˆ—ï¼š{', '.join(missing_cols)}")
                else:
                    st.success("âœ… æ•°æ®æ ¼å¼æ£€æŸ¥é€šè¿‡ï¼æ‰€æœ‰å¿…è¦åˆ—éƒ½å·²å­˜åœ¨ã€‚")
                    
                # æ£€æŸ¥æ•°æ®ç±»å‹
                for col in [c for c in required_cols if c in preview_df.columns]:
                    if not pd.api.types.is_numeric_dtype(preview_df[col]):
                        st.warning(f"âš ï¸ è­¦å‘Šï¼š'{col}' åˆ—ä¸æ˜¯æ•°å€¼ç±»å‹ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´è®­ç»ƒå¤±è´¥ã€‚")
                
                # æ£€æŸ¥ç¼ºå¤±å€¼
                if preview_df[preview_df.columns.intersection(required_cols)].isnull().any().any():
                    st.warning("âš ï¸ è­¦å‘Šï¼šæ•°æ®ä¸­å­˜åœ¨ç¼ºå¤±å€¼ï¼Œè¿™å¯èƒ½ä¼šå½±å“æ¨¡å‹è®­ç»ƒæ•ˆæœã€‚")
                    
            except Exception as e:
                st.error(f"é¢„è§ˆæ•°æ®æ—¶å‡ºé”™ï¼š{e}")
        
        # è®­ç»ƒæŒ‰é’®
        if st.button(f"ğŸš€ å¼€å§‹è®­ç»ƒ {model_type} æ¨¡å‹ ({estimated_time})", use_container_width=True):
            try:
                # è¯»å–æ•°æ®
                uploaded_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°æ–‡ä»¶å¼€å¤´
                df = pd.read_csv(uploaded_file)
                st.info(f"ğŸ“Š å·²åŠ è½½æ•°æ®ï¼š{len(df)}è¡Œ x {len(df.columns)}åˆ—")
                
                # æ£€æŸ¥ç›®æ ‡åˆ—æ˜¯å¦å­˜åœ¨
                if model_type not in df.columns:
                    st.error(f"ä¸Šä¼ çš„æ–‡ä»¶ä¸­ç¼ºå°‘ç›®æ ‡åˆ— '{model_type}'ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹ã€‚")
                else:
                    # æ•°æ®é¢„å¤„ç†
                    with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®..."):
                        processed_df, scaler = process_data_for_training(df)
                    
                    if processed_df is not None:
                        # å®šä¹‰ç‰¹å¾å’Œç›®æ ‡
                        features = ['è€åŒ–æ¸©åº¦_normalized', 'è€åŒ–æ—¶é—´_normalized', 'è€åŒ–æ¹¿åº¦_normalized', 'æœ‰æ— ç†”æ¥ç—•(0/1)']
                        target = model_type
                        
                        # è®­ç»ƒå¹¶è·å–ç»“æœ
                        model, fig, metrics, grid_resolution = train_and_visualize_model(
                            processed_df, features, target, model_type, mode=training_mode
                        )
                        
                        if model:
                            st.success(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼ä½¿ç”¨äº†{training_mode}æ¨¡å¼ã€‚")
                            
                            # ä¿å­˜ç»“æœåˆ° session_state ä»¥ä¾¿åç»­ä½¿ç”¨
                            st.session_state['trained_model'] = model
                            st.session_state['scaler'] = scaler
                            st.session_state['figure'] = fig
                            st.session_state['metrics'] = metrics
                            st.session_state['model_type'] = model_type
                            st.session_state['features'] = features
                            st.session_state['grid_resolution'] = grid_resolution
                            st.session_state['training_mode'] = training_mode

            except Exception as e:
                    st.error(f"å¤„ç†æ–‡ä»¶æˆ–è®­ç»ƒæ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    # 3. å±•ç¤ºè®­ç»ƒç»“æœ
    if 'figure' in st.session_state and st.session_state['model_type'] == model_type:
        st.header("ğŸ“Š è®­ç»ƒç»“æœä¸åˆ†æ")
        
        # å¸ƒå±€
        col1, col2 = st.columns([2.5, 1.5])

        with col1:
            st.subheader("ä¸‰ç»´å“åº”é¢ä¸æ•°æ®åˆ†å¸ƒ")
            st.plotly_chart(st.session_state['figure'], use_container_width=True)
            with st.expander("å›¾è¡¨è§£è¯»"):
                st.markdown(
                    """
                    è¿™ä¸ª3Då›¾è¡¨å±•ç¤ºäº†æ¨¡å‹å­¦ä¹ åˆ°çš„è§„å¾‹ï¼š
                    - **æ•£ç‚¹**: ä»£è¡¨æ‚¨çš„åŸå§‹æ•°æ®ç‚¹ã€‚è“è‰²è¡¨ç¤ºæ— ç†”æ¥ç—•æ ·æœ¬ï¼Œçº¢è‰²è¡¨ç¤ºæœ‰ç†”æ¥ç—•æ ·æœ¬ã€‚
                    - **å½©è‰²æ›²é¢**: è¿™æ˜¯æ¨¡å‹ç”Ÿæˆçš„â€œå“åº”é¢â€ï¼Œå®ƒé¢„æµ‹äº†åœ¨ä¸åŒ`è€åŒ–æ¸©åº¦`å’Œ`è€åŒ–æ—¶é—´`ç»„åˆä¸‹çš„ææ–™å¼ºåº¦ã€‚
                      - **è“è‰²æ›²é¢** å¯¹åº” **æ— ç†”æ¥ç—•** çš„æƒ…å†µã€‚
                      - **çº¢è‰²æ›²é¢** å¯¹åº” **æœ‰ç†”æ¥ç—•** çš„æƒ…å†µã€‚
                    - **äº¤äº’æ“ä½œ**: æ‚¨å¯ä»¥æ‹–åŠ¨å›¾è¡¨è¿›è¡Œæ—‹è½¬ï¼Œæ»šåŠ¨é¼ æ ‡æ»šè½®è¿›è¡Œç¼©æ”¾ï¼Œä»¥ä¾¿ä»ä¸åŒè§’åº¦è§‚å¯Ÿæ•°æ®å’Œæ¨¡å‹æ›²é¢ã€‚
                    - **åˆ†æ**: é€šè¿‡è§‚å¯Ÿæ›²é¢çš„é«˜ä½å’Œèµ°åŠ¿ï¼Œå¯ä»¥ç›´è§‚åœ°åˆ¤æ–­å‡ºå“ªäº›å·¥è‰ºå‚æ•°ç»„åˆèƒ½å¸¦æ¥æ›´é«˜çš„ææ–™å¼ºåº¦ã€‚
                    """
                )

        with col2:
            st.subheader("æ¨¡å‹æ€§èƒ½è¯„ä¼°")
            metrics = st.session_state['metrics']
            st.metric(label="RÂ² (å†³å®šç³»æ•°)", value=f"{metrics['r2']:.4f}", help="è¶Šæ¥è¿‘1è¶Šå¥½ï¼Œè¡¨ç¤ºæ¨¡å‹å¯¹æ•°æ®å˜å¼‚æ€§çš„è§£é‡Šèƒ½åŠ›è¶Šå¼ºã€‚")
            st.metric(label="MAE (å¹³å‡ç»å¯¹è¯¯å·®)", value=f"{metrics['mae']:.4f}", help=f"è¡¨ç¤ºé¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å¹³å‡å·®å¼‚å¤§å°ã€‚æ­¤æ¨¡å‹çš„å¹³å‡é¢„æµ‹è¯¯å·®ä¸º {metrics['mae']:.4f}ã€‚")
            
            with st.expander("è¯„ä¼°æŒ‡æ ‡çš„æ„ä¹‰", expanded=True):
                 st.markdown(
                    f"""
                    #### **RÂ² (R-squared / å†³å®šç³»æ•°)**
                    - **æ˜¯ä»€ä¹ˆ?** RÂ²åˆ†æ•°è¡¡é‡çš„æ˜¯æ¨¡å‹å¯¹æ•°æ®å˜åŒ–çš„è§£é‡Šç¨‹åº¦ã€‚å®ƒçš„å–å€¼èŒƒå›´é€šå¸¸åœ¨0åˆ°1ä¹‹é—´ã€‚
                    - **å¦‚ä½•è§£è¯»?**
                        - **RÂ² = 1**: å®Œç¾æ¨¡å‹ï¼Œæ¨¡å‹è§£é‡Šäº†æ•°æ®ä¸­100%çš„å˜åŒ–ã€‚
                        - **RÂ² = {metrics['r2']:.2f}**: å½“å‰æ¨¡å‹å¯ä»¥è§£é‡Šçº¦ **{metrics['r2']*100:.1f}%** çš„æ•°æ®æ–¹å·®ã€‚è¿™æ˜¯ä¸€ä¸ªç›¸å½“ä¸é”™çš„æ€§èƒ½ï¼Œè¯´æ˜æ¨¡å‹å¾ˆå¥½åœ°æ•æ‰äº†å…³é”®å˜é‡ä¹‹é—´çš„å…³ç³»ã€‚
                        - **RÂ² = 0**: æ¨¡å‹æ€§èƒ½ç­‰åŒäºä¸€ä¸ªç®€å•çš„å¹³å‡å€¼é¢„æµ‹ï¼Œæ²¡æœ‰å­¦åˆ°ä»»ä½•æœ‰æ•ˆä¿¡æ¯ã€‚

                    #### **MAE (Mean Absolute Error / å¹³å‡ç»å¯¹è¯¯å·®)**
                    - **æ˜¯ä»€ä¹ˆ?** MAEè®¡ç®—çš„æ˜¯æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹å·®çš„ç»å¯¹å€¼çš„å¹³å‡æ•°ã€‚
                    - **å¦‚ä½•è§£è¯»?**
                        - å®ƒçš„å•ä½ä¸æ‚¨çš„ç›®æ ‡å€¼ï¼ˆ{model_type}ï¼‰ç›¸åŒã€‚
                        - **MAE = {metrics['mae']:.2f}**: æ„å‘³ç€æ¨¡å‹çš„é¢„æµ‹ç»“æœå¹³å‡ä¼šä¸çœŸå®å€¼ç›¸å·®çº¦ **{metrics['mae']:.2f}**ã€‚è¿™ä¸ªå€¼è¶Šå°ï¼Œä»£è¡¨æ¨¡å‹çš„é¢„æµ‹è¶Šç²¾å‡†ã€‚
                    """
                )
        
        # 4. ä¿å­˜æ¨¡å‹
        st.subheader("æ­¥éª¤ 2: ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹")
        st.info("æ‚¨å¯ä»¥å°†å½“å‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆåŒ…æ‹¬æ•°æ®å½’ä¸€åŒ–è§„åˆ™ï¼‰ä¿å­˜ä¸‹æ¥ï¼Œä»¥ä¾¿æœªæ¥ç›´æ¥åŠ è½½ä½¿ç”¨ã€‚")
        
        # å°†æ¨¡å‹å’Œscaleræ‰“åŒ…
        model_to_save = {
            'model': st.session_state['trained_model'],
            'scaler': st.session_state['scaler'],
            'model_type': st.session_state['model_type'],
            'features': st.session_state['features']
        }
        
        # ä½¿ç”¨BytesIOåœ¨å†…å­˜ä¸­åˆ›å»ºæ–‡ä»¶
        buffer = BytesIO()
        joblib.dump(model_to_save, buffer)
        buffer.seek(0)

        st.download_button(
            label=f"ğŸ“¥ ä¸‹è½½ {model_type} æ¨¡å‹æ–‡ä»¶ (.joblib)",
            data=buffer,
            file_name=f"{model_type}_model.joblib",
            mime="application/octet-stream",
            use_container_width=True
        )

# ==============================================================================
# "åŠ è½½æ¨¡å‹å¹¶é¢„æµ‹" é¡µé¢
# ==============================================================================
elif app_mode == "åŠ è½½æ¨¡å‹å¹¶é¢„æµ‹":
    st.header("æ¨¡å¼äºŒ: åŠ è½½å·²æœ‰æ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹")
    
    st.subheader("æ­¥éª¤ 1: ä¸Šä¼ æ‚¨çš„æ¨¡å‹æ–‡ä»¶")
    uploaded_model_file = st.file_uploader(
        "è¯·ä¸Šä¼ ä¹‹å‰ä¿å­˜çš„ .joblib æ¨¡å‹æ–‡ä»¶", 
        type="joblib"
    )

    if uploaded_model_file:
        try:
            # åŠ è½½æ¨¡å‹å’Œscaler
            model_data = joblib.load(uploaded_model_file)
            model = model_data['model']
            scaler = model_data['scaler']
            loaded_model_type = model_data['model_type']
            features = model_data['features']
            
            st.success(f"æˆåŠŸåŠ è½½ **{loaded_model_type}** æ¨¡å‹ï¼ç°åœ¨å¯ä»¥è¿›è¡Œé¢„æµ‹ã€‚")
            
            # --- é¢„æµ‹ç•Œé¢ ---
            st.subheader("æ­¥éª¤ 2: è¾“å…¥å‚æ•°è¿›è¡Œé¢„æµ‹")
            st.warning("è¯·ç¡®ä¿è¾“å…¥çš„å‚æ•°ä¸åŠ è½½çš„æ¨¡å‹ç±»å‹ç›¸åŒ¹é…ã€‚")
            
            form = st.form(key='prediction_form')
            with form:
                col1, col2, col3 = st.columns(3)
                with col1:
                    temp = st.number_input("è€åŒ–æ¸©åº¦ (Â°C)", min_value=0.0, max_value=200.0, value=85.0, step=0.5)
                with col2:
                    humidity = st.number_input("è€åŒ–æ¹¿åº¦ (%)", min_value=0.0, max_value=100.0, value=75.0, step=0.5)
                with col3:
                    time = st.number_input("è€åŒ–æ—¶é—´ (å°æ—¶)", min_value=0.0, max_value=1000.0, value=24.0, step=1.0)
                
                weld_status = st.selectbox("æœ‰æ— ç†”æ¥ç—•", options=[0, 1], format_func=lambda x: "æ— ç†”æ¥ç—•" if x == 0 else "æœ‰ç†”æ¥ç—•")
                
                submit_button = st.form_submit_button(label="ğŸ”® è¿è¡Œé¢„æµ‹")

            if submit_button:
                # å‡†å¤‡è¾“å…¥æ•°æ®
                raw_input_data = pd.DataFrame([[temp, humidity, time]], columns=['è€åŒ–æ¸©åº¦', 'è€åŒ–æ¹¿åº¦', 'è€åŒ–æ—¶é—´'])
                
                # ä½¿ç”¨åŠ è½½çš„scalerè¿›è¡Œå½’ä¸€åŒ–
                normalized_input = scaler.transform(raw_input_data)
                
                # ç»„åˆæ‰€æœ‰ç‰¹å¾
                final_input = pd.DataFrame(normalized_input, columns=['è€åŒ–æ¸©åº¦_normalized', 'è€åŒ–æ¹¿åº¦_normalized', 'è€åŒ–æ—¶é—´_normalized'])
                final_input['æœ‰æ— ç†”æ¥ç—•(0/1)'] = weld_status
                
                # ç¡®ä¿åˆ—é¡ºåºä¸è®­ç»ƒæ—¶ä¸€è‡´
                final_input = final_input[features]

                # è¿›è¡Œé¢„æµ‹
                prediction = model.predict(final_input)
                
                st.success(f"é¢„æµ‹çš„ **{loaded_model_type}** ç»“æœä¸º:")
                st.metric(label=loaded_model_type, value=f"{prediction[0]:.2f}")

        except Exception as e:
            st.error(f"åŠ è½½æ¨¡å‹æˆ–é¢„æµ‹æ—¶å‘ç”Ÿé”™è¯¯: {e}. è¯·ç¡®ä¿ä¸Šä¼ äº†æ­£ç¡®çš„æ¨¡å‹æ–‡ä»¶ã€‚")